{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 8.453975345608463
  },
  "request_latency": {
    "unit": "ms",
    "avg": 5912.076811433333,
    "p25": 5888.359915749999,
    "p50": 5934.0562155,
    "p75": 5962.81728375,
    "p90": 5975.2215203,
    "p95": 5975.4745821,
    "p99": 5975.67697367,
    "min": 5755.991088,
    "max": 5976.301157,
    "std": 56.4822634878656
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 804.11762885,
    "p25": 413.75499625,
    "p50": 991.433329,
    "p75": 1021.69721225,
    "p90": 1024.5970392,
    "p95": 1024.87549275,
    "p99": 1038.23661086,
    "min": 39.85047,
    "max": 1038.63657,
    "std": 329.06643887499274
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 21.95435303,
    "p25": 20.990015,
    "p50": 21.408853,
    "p75": 22.827962499999998,
    "p90": 24.386339899999996,
    "p95": 25.0082353,
    "p99": 25.54999654,
    "min": 17.961316,
    "max": 26.848177999999997,
    "std": 1.5072962132092538
  },
  "output_token_throughput": {
    "unit": "requests/sec",
    "avg": 1977.3002935843635
  },
  "output_token_throughput_per_request": {
    "unit": "requests/sec",
    "avg": 39.565690113476705,
    "p25": 38.7592399556885,
    "p50": 39.46565153564677,
    "p75": 40.11231793108199,
    "p90": 40.89714962401291,
    "p95": 41.609118308546286,
    "p99": 45.15169141564404,
    "min": 35.725245679348745,
    "max": 47.386795848887694,
    "std": 1.4462444049424377
  },
  "output_sequence_length": {
    "unit": "requests/sec",
    "avg": 233.89,
    "p25": 229.0,
    "p50": 233.0,
    "p75": 237.0,
    "p90": 241.0,
    "p95": 244.0,
    "p99": 261.1299999999999,
    "min": 213.0,
    "max": 274.0,
    "std": 8.116930864967784
  },
  "input_sequence_length": {
    "unit": "requests/sec",
    "avg": 200.42666666666668,
    "p25": 193.75,
    "p50": 200.0,
    "p75": 207.5,
    "p90": 214.0,
    "p95": 216.0,
    "p99": 218.0,
    "min": 178.0,
    "max": 221.0,
    "std": 9.681147774010178
  },
  "input_config": {
    "subcommand": "profile",
    "model": [
      "mark_test"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "server_metrics_url": null,
    "streaming": true,
    "u": "localhost:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 200,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 10,
    "goodput": null,
    "image_width_mean": 100,
    "image_width_stddev": 0,
    "image_height_mean": 100,
    "image_height_stddev": 0,
    "image_format": null,
    "concurrency": 50,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/mark_test-openai-chat-concurrency50",
    "generate_plots": false,
    "profile_export_file": "artifacts/mark_test-openai-chat-concurrency50/profile_export.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "tokenizer_revision": "main",
    "tokenizer_trust_remote_code": false,
    "verbose": false,
    "prompt_source": "synthetic",
    "formatted_model_name": "mark_test",
    "extra_inputs": {
      "max_tokens": 200,
      "min_tokens": 200,
      "ignore_eos": true
    }
  }
}