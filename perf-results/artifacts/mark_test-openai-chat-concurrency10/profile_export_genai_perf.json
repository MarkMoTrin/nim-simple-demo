{
  "request_throughput": {
    "unit": "requests/sec",
    "avg": 2.171304623945151
  },
  "request_latency": {
    "unit": "ms",
    "avg": 4602.060807514285,
    "p25": 4578.3919005,
    "p50": 4598.198416,
    "p75": 4624.6522079999995,
    "p90": 4639.257355,
    "p95": 4639.29871335,
    "p99": 4639.413240319999,
    "min": 4566.361879,
    "max": 4639.413359,
    "std": 22.780505461049874
  },
  "time_to_first_token": {
    "unit": "ms",
    "avg": 175.06076704285712,
    "p25": 172.60462149999998,
    "p50": 186.41055749999998,
    "p75": 188.76953724999998,
    "p90": 226.851016,
    "p95": 227.01780569999997,
    "p99": 227.07767368,
    "min": 43.933192999999996,
    "max": 227.095219,
    "std": 45.91380595273277
  },
  "inter_token_latency": {
    "unit": "ms",
    "avg": 18.998680357142856,
    "p25": 18.72166,
    "p50": 19.0364145,
    "p75": 19.337758,
    "p90": 19.7963046,
    "p95": 20.238214499999998,
    "p99": 20.52436212,
    "min": 16.219409,
    "max": 20.682546,
    "std": 0.7508406952016241
  },
  "output_token_throughput": {
    "unit": "requests/sec",
    "avg": 508.8607479402885
  },
  "output_token_throughput_per_request": {
    "unit": "requests/sec",
    "avg": 50.926406448940135,
    "p25": 49.8137254778605,
    "p50": 50.734603254231565,
    "p75": 51.546482942165795,
    "p90": 52.656899891504764,
    "p95": 53.25459349060042,
    "p99": 57.718753758548864,
    "min": 46.573215994233664,
    "max": 59.58861261899921,
    "std": 2.0015951053510035
  },
  "output_sequence_length": {
    "unit": "requests/sec",
    "avg": 234.35714285714286,
    "p25": 229.0,
    "p50": 234.0,
    "p75": 237.75,
    "p90": 242.1,
    "p95": 244.55,
    "p99": 265.03000000000003,
    "min": 213.0,
    "max": 274.0,
    "std": 9.033328312873572
  },
  "input_sequence_length": {
    "unit": "requests/sec",
    "avg": 200.7,
    "p25": 194.0,
    "p50": 200.0,
    "p75": 208.5,
    "p90": 212.1,
    "p95": 215.55,
    "p99": 218.0,
    "min": 180.0,
    "max": 218.0,
    "std": 9.054359644471196
  },
  "input_config": {
    "subcommand": "profile",
    "model": [
      "mark_test"
    ],
    "model_selection_strategy": "round_robin",
    "backend": "tensorrtllm",
    "endpoint": "v1/chat/completions",
    "endpoint_type": "chat",
    "service_kind": "openai",
    "server_metrics_url": null,
    "streaming": true,
    "u": "localhost:8000",
    "batch_size": 1,
    "input_dataset": null,
    "num_prompts": 100,
    "output_tokens_mean": 200,
    "output_tokens_mean_deterministic": false,
    "output_tokens_stddev": 0,
    "random_seed": 0,
    "synthetic_input_tokens_mean": 200,
    "synthetic_input_tokens_stddev": 10,
    "goodput": null,
    "image_width_mean": 100,
    "image_width_stddev": 0,
    "image_height_mean": 100,
    "image_height_stddev": 0,
    "image_format": null,
    "concurrency": 10,
    "measurement_interval": 10000,
    "request_rate": null,
    "stability_percentage": 999,
    "artifact_dir": "artifacts/mark_test-openai-chat-concurrency10",
    "generate_plots": false,
    "profile_export_file": "artifacts/mark_test-openai-chat-concurrency10/profile_export.json",
    "tokenizer": "hf-internal-testing/llama-tokenizer",
    "tokenizer_revision": "main",
    "tokenizer_trust_remote_code": false,
    "verbose": false,
    "prompt_source": "synthetic",
    "formatted_model_name": "mark_test",
    "extra_inputs": {
      "max_tokens": 200,
      "min_tokens": 200,
      "ignore_eos": true
    }
  }
}